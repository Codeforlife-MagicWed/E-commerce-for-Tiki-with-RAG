{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNbhRSmaG-1"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers faiss-cpu ujson tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWAcVGq5LXcv"
      },
      "outputs": [],
      "source": [
        "!pip install -U unsloth transformers accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIefxaX14TD6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6QpjZsMMOuq"
      },
      "outputs": [],
      "source": [
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:true\")\n",
        "\n",
        "faq_input      = \"\"\n",
        "product_input  = \"\"\n",
        "\n",
        "faq_out_parquet     = \"\"\n",
        "faq_out_npy         = \"\"\n",
        "product_out_parquet = \"\"\n",
        "product_out_npy     = \"\"\n",
        "\n",
        "MODEL_NAME      = \"BAAI/bge-m3\"\n",
        "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE_INIT = 64\n",
        "SHARD_SIZE      = 2000\n",
        "NORMALIZE       = True\n",
        "MAX_SEQ_LENGTH  = 512\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "def load_any(path):\n",
        "    path_l = path.lower()\n",
        "    if path_l.endswith(\".jsonl\"):\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    rows.append(json.loads(line))\n",
        "        return rows\n",
        "    elif path_l.endswith(\".json\"):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "        if isinstance(data, dict):\n",
        "            if \"items\" in data and isinstance(data[\"items\"], list):\n",
        "                return data[\"items\"]\n",
        "            try:\n",
        "                return list(data.values())\n",
        "            except Exception:\n",
        "                return [data]\n",
        "    raise ValueError(f\"Unsupported file type: {path}\")\n",
        "\n",
        "def safe_encode(model, texts, batch_size, normalize=True):\n",
        "    bs = batch_size\n",
        "    while True:\n",
        "        try:\n",
        "            vecs = model.encode(\n",
        "                texts,\n",
        "                batch_size=bs,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=normalize\n",
        "            )\n",
        "            return vecs, bs\n",
        "        except RuntimeError as e:\n",
        "            if \"cuda out of memory\" in str(e).lower() and bs > 4 and DEVICE == \"cuda\":\n",
        "                bs = max(4, bs // 2)\n",
        "                torch.cuda.empty_cache()\n",
        "                print(f\"OOM - giảm batch_size xuống {bs}\")\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "def _meta_to_json(x):\n",
        "    if isinstance(x, (dict, list)):\n",
        "        return json.dumps(x, ensure_ascii=False)\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def _coerce_df_strings(df_sh):\n",
        "    for c in [\"id\",\"type\",\"url\",\"category\",\"title\",\"text\",\"meta\"]:\n",
        "        if c not in df_sh.columns:\n",
        "            df_sh[c] = None\n",
        "    df_sh[\"meta\"] = df_sh[\"meta\"].apply(_meta_to_json)\n",
        "    for c in [\"id\",\"type\",\"url\",\"category\",\"title\",\"text\"]:\n",
        "        df_sh[c] = df_sh[c].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
        "    return df_sh\n",
        "\n",
        "PARQUET_SCHEMA = pa.schema([\n",
        "    pa.field(\"id\",       pa.string()),\n",
        "    pa.field(\"type\",     pa.string()),\n",
        "    pa.field(\"url\",      pa.string()),\n",
        "    pa.field(\"category\", pa.string()),\n",
        "    pa.field(\"title\",    pa.string()),\n",
        "    pa.field(\"text\",     pa.string()),\n",
        "    pa.field(\"meta\",     pa.string()),\n",
        "    pa.field(\"embedding\", pa.list_(pa.float32())),\n",
        "])\n",
        "\n",
        "def embed_streaming(INPUT_PATH, OUT_PARQUET, OUT_NPY, model):\n",
        "    rows = load_any(INPUT_PATH)\n",
        "    df = pd.DataFrame(rows)\n",
        "    if \"text\" not in df.columns:\n",
        "        raise ValueError(f\"Thiếu cột 'text' trong dữ liệu: {INPUT_PATH}\")\n",
        "\n",
        "    print(f\"Loading: {INPUT_PATH} | Rows: {len(df)}\")\n",
        "\n",
        "    try:\n",
        "        model.max_seq_length = MAX_SEQ_LENGTH\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    N = len(df)\n",
        "    writer = None\n",
        "    memmap = None\n",
        "    dim = None\n",
        "    current_offset = 0\n",
        "    batch_size = BATCH_SIZE_INIT\n",
        "\n",
        "    for start in tqdm(range(0, N, SHARD_SIZE), desc=f\"Embedding shards {os.path.basename(OUT_PARQUET)}\"):\n",
        "        end = min(N, start + SHARD_SIZE)\n",
        "        df_sh = df.iloc[start:end].copy()\n",
        "        texts = df_sh[\"text\"].astype(str).tolist()\n",
        "\n",
        "        vecs, batch_size = safe_encode(model, texts, batch_size, normalize=NORMALIZE)\n",
        "\n",
        "        if dim is None:\n",
        "            dim = vecs.shape[1]\n",
        "            memmap = np.memmap(OUT_NPY, dtype=np.float32, mode=\"w+\", shape=(N, dim))\n",
        "            writer = pq.ParquetWriter(OUT_PARQUET, schema=PARQUET_SCHEMA, compression=\"zstd\")\n",
        "            print(f\"embedding_dim = {dim}\")\n",
        "            print(f\"Init writers {OUT_PARQUET} | {OUT_NPY}\")\n",
        "\n",
        "        memmap[current_offset:current_offset + len(df_sh)] = vecs.astype(np.float32)\n",
        "\n",
        "        df_sh = _coerce_df_strings(df_sh)\n",
        "        emb_list = [v.astype(np.float32).tolist() for v in vecs]\n",
        "        table = pa.Table.from_arrays(\n",
        "            [\n",
        "                pa.array(df_sh[\"id\"],       type=pa.string()),\n",
        "                pa.array(df_sh[\"type\"],     type=pa.string()),\n",
        "                pa.array(df_sh[\"url\"],      type=pa.string()),\n",
        "                pa.array(df_sh[\"category\"], type=pa.string()),\n",
        "                pa.array(df_sh[\"title\"],    type=pa.string()),\n",
        "                pa.array(df_sh[\"text\"],     type=pa.string()),\n",
        "                pa.array(df_sh[\"meta\"],     type=pa.string()),\n",
        "                pa.array(emb_list,          type=pa.list_(pa.float32())),\n",
        "            ],\n",
        "            names=[\"id\",\"type\",\"url\",\"category\",\"title\",\"text\",\"meta\",\"embedding\"]\n",
        "        )\n",
        "        writer.write_table(table)\n",
        "\n",
        "        current_offset += len(df_sh)\n",
        "\n",
        "        del df_sh, texts, vecs, table, emb_list\n",
        "        if DEVICE == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "    if memmap is not None:\n",
        "        del memmap\n",
        "\n",
        "    print(f\"DONE {OUT_PARQUET} | {OUT_NPY}  (rows={N}, dim={dim})\")\n",
        "\n",
        "for p in [faq_out_parquet, faq_out_npy, product_out_parquet, product_out_npy]:\n",
        "    try:\n",
        "        os.remove(p)\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "print(f\"Loaded model: {MODEL_NAME} | max_seq_length={getattr(model,'max_seq_length','N/A')}\")\n",
        "\n",
        "embed_streaming(faq_input, faq_out_parquet, faq_out_npy, model)\n",
        "embed_streaming(product_input, product_out_parquet, product_out_npy, model)\n",
        "\n",
        "print(\"All done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUzMubuEMZS"
      },
      "outputs": [],
      "source": [
        "faq_df = pd.read_parquet(\"faq_vectors_bge_new.parquet\")\n",
        "E_faq = np.vstack(faq_df[\"embedding\"].to_numpy()).astype(\"float32\")\n",
        "np.save(\"faq_vectors_bge.npy\", E_faq)\n",
        "\n",
        "prod_df = pd.read_parquet(\"product_vectors_bge.parquet\")\n",
        "E_prod = np.vstack(prod_df[\"embedding\"].to_numpy()).astype(\"float32\")\n",
        "np.save(\"product_vectors_bge.npy\", E_prod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_86tTqYHAJyY"
      },
      "outputs": [],
      "source": [
        "faq_parquet = \"faq_vectors_bge.parquet\"\n",
        "faq_npy     = \"faq_vectors_bge_fixed.npy\"\n",
        "\n",
        "product_parquet = \"product_vectors_bge.parquet\"\n",
        "product_npy     = \"product_vectors_bge.npy\"\n",
        "\n",
        "\n",
        "faq_out_parquet     = \"faq_vectors_bge_new.parquet\"\n",
        "faq_out_npy         = \"faq_vectors_bge_new.npy\"\n",
        "product_out_parquet = \"product_vectors_bge.parquet\"\n",
        "product_out_npy     = \"product_vectors_bge.npy\"\n",
        "\n",
        "for p in [faq_parquet, faq_npy, product_parquet, product_npy]:\n",
        "    print(p, \" \" if os.path.exists(p) else \" \")\n",
        "from numpy.linalg import norm\n",
        "\n",
        "faq_df  = pd.read_parquet(faq_parquet)\n",
        "E_faq   = np.load(faq_npy, allow_pickle=True)\n",
        "prod_df = pd.read_parquet(product_parquet)\n",
        "E_prod  = np.load(product_npy, allow_pickle=True)\n",
        "\n",
        "print(f\"FAQ shape:     {E_faq.shape}\")\n",
        "print(f\"Product shape: {E_prod.shape}\")\n",
        "\n",
        "L2_faq  = norm(E_faq, axis=1)\n",
        "L2_prod = norm(E_prod, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05xM0-dvFGCP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
