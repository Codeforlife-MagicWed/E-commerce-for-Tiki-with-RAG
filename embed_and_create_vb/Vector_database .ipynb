{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc3k2xDse7z7",
        "outputId": "f46bad13-f29f-494b-f59c-9e2dcb89b0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/337.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m327.7/337.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q qdrant-client pandas pyarrow numpy tqdm sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJTQp4ycQxpQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from qdrant_client.models import Batch\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "from tqdm import trange\n",
        "import numpy as np, pandas as pd, uuid, math, json, sys, traceback, re\n",
        "from qdrant_client.models import Distance, VectorParams, HnswConfigDiff, OptimizersConfigDiff\n",
        "import pandas as pd\n",
        "from qdrant_client.models import PayloadSchemaType\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GGXLRRietkS",
        "outputId": "6238fb52-e488-4651-a323-0afa3f1ccebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collections=[CollectionDescription(name='product_bge'), CollectionDescription(name='faq_bge')]\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "client = QdrantClient(\n",
        "    url=\"XXXXXXX\",\n",
        "    api_key=\"XXXXXXX\",\n",
        ")\n",
        "\n",
        "print(client.get_collections())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-j-punQ0Ry"
      },
      "source": [
        "## Upsert product and faq collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uo7WkT-frnV",
        "outputId": "9e93dad7-040c-4bc7-eaad-65963681b9cc"
      },
      "outputs": [],
      "source": [
        "faq_parquet     = \"faq_vectors_bge.parquet\"\n",
        "product_parquet = \"product_vectors_bge.parquet\"\n",
        "\n",
        "faq_df  = pd.read_parquet(faq_parquet)\n",
        "prod_df = pd.read_parquet(product_parquet)\n",
        "\n",
        "def get_dim(df):\n",
        "    assert len(df) > 0, \"DataFrame trống!\"\n",
        "    return len(df.iloc[0][\"embedding\"])\n",
        "\n",
        "DIM = get_dim(faq_df if len(faq_df) else prod_df)\n",
        "print(f\" DIM = {DIM}, faq={len(faq_df)} rows, product={len(prod_df)} rows\")\n",
        "\n",
        "def ensure_collection(name: str, dim: int):\n",
        "    if client.collection_exists(name):\n",
        "        print(f\"Collection '{name}' đã tồn tại.\")\n",
        "        return\n",
        "\n",
        "    client.create_collection(\n",
        "        collection_name=name,\n",
        "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
        "        hnsw_config=HnswConfigDiff(m=48, ef_construct=200),\n",
        "        optimizers_config=OptimizersConfigDiff(indexing_threshold=20000),\n",
        "    )\n",
        "    print(f\" Created collection: {name}\")\n",
        "\n",
        "ensure_collection(\"faq_bge\", DIM)\n",
        "ensure_collection(\"product_bge\", DIM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoCM98z8gC5z"
      },
      "outputs": [],
      "source": [
        "def as_str(x, default=\"\"):\n",
        "    if x is None:\n",
        "        return default\n",
        "    if isinstance(x, float) and math.isnan(x):\n",
        "        return default\n",
        "    return x if isinstance(x, str) else str(x)\n",
        "\n",
        "def to_uuid5_list(series, prefix):\n",
        "    vals = [as_str(v) for v in series]\n",
        "    return [str(uuid.uuid5(uuid.NAMESPACE_URL, f\"{prefix}:{v}\")) for v in vals]\n",
        "\n",
        "def ensure_meta_dict(m):\n",
        "    if isinstance(m, dict):\n",
        "        return m\n",
        "    if isinstance(m, str):\n",
        "        try:\n",
        "            return json.loads(m)\n",
        "        except json.JSONDecodeError:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def ensure_matrix(col):\n",
        "    if isinstance(col, np.ndarray) and col.ndim == 2:\n",
        "        return col.astype(\"float32\")\n",
        "    arr = np.vstack(col.to_numpy()).astype(\"float32\")\n",
        "    return arr\n",
        "\n",
        "BRAND_RX = re.compile(r\"thương\\s*hiệu\\s*:\\s*([A-Za-z0-9À-ỹ\\-\\&\\.\\s]+)\", re.I)\n",
        "\n",
        "def extract_brand(r, meta_data):\n",
        "    b = r.get(\"brand\")\n",
        "    if isinstance(b, str) and b.strip():\n",
        "        return b.strip(), \"top\"\n",
        "    b = meta_data.get(\"brand\")\n",
        "    if isinstance(b, str) and b.strip():\n",
        "        return b.strip(), \"meta\"\n",
        "    txt = as_str(r.get(\"text\", \"\"))\n",
        "    m = BRAND_RX.search(txt)\n",
        "    if m:\n",
        "        return m.group(1).strip(\" ;,.\\n\"), \"regex\"\n",
        "    return None, None\n",
        "\n",
        "def extract_thumbnail(r, meta_data):\n",
        "    th = r.get(\"thumbnail\")\n",
        "    if isinstance(th, str) and th.strip():\n",
        "        return th.strip()\n",
        "    th = meta_data.get(\"thumbnail\") or meta_data.get(\"image\") or meta_data.get(\"img\")\n",
        "    if isinstance(th, str) and th.strip():\n",
        "        return th.strip()\n",
        "    return None\n",
        "\n",
        "def resolve_parent_uid(r, meta_data):\n",
        "    pu = r.get(\"parent_uid\")\n",
        "    if isinstance(pu, str) and pu.strip():\n",
        "        return pu.strip()\n",
        "\n",
        "    id_chunk = as_str(r.get(\"id\", \"\"))\n",
        "    if id_chunk and id_chunk.count(\"::\") >= 2:\n",
        "        return id_chunk.rsplit(\"::\", 1)[0]\n",
        "    elif id_chunk and id_chunk.count(\"::\") == 1:\n",
        "        return id_chunk\n",
        "\n",
        "    sid = meta_data.get(\"source_id\")\n",
        "    if sid:\n",
        "        return f\"prod_{sid}\"\n",
        "    return \"\"\n",
        "\n",
        "def make_payloads(df_part, id_col=\"id\"):\n",
        "    payloads = []\n",
        "    for _, r in df_part.iterrows():\n",
        "        meta_data = ensure_meta_dict(r.get(\"meta\", {}))\n",
        "\n",
        "        parent_uid = resolve_parent_uid(r, meta_data)\n",
        "        brand, brand_src = extract_brand(r, meta_data)\n",
        "        thumbnail = extract_thumbnail(r, meta_data)\n",
        "\n",
        "        external_id = as_str(r.get(id_col, \"\"))\n",
        "\n",
        "        p = {\n",
        "            \"id\":          external_id,\n",
        "            \"type\":        as_str(r.get(\"type\",\"\")),\n",
        "            \"title\":       as_str(r.get(\"title\",\"\"))[:512],\n",
        "            \"url\":         as_str(r.get(\"url\",\"\"))[:1024],\n",
        "            \"category\":    as_str(r.get(\"category\",\"\"))[:256],\n",
        "            \"text\":        as_str(r.get(\"text\",\"\")),\n",
        "            \"parent_uid\":  parent_uid,\n",
        "            \"brand\":       brand or \"\",\n",
        "            \"thumbnail\":   thumbnail or \"\",\n",
        "            \"brand_source\": brand_src or \"\",\n",
        "            \"price_numeric\": (meta_data.get(\"price_numeric\")),\n",
        "            \"rating\":        (meta_data.get(\"rating\")),\n",
        "            \"review_count\":  (meta_data.get(\"review_count\")),\n",
        "        }\n",
        "        payloads.append(p)\n",
        "    return payloads\n",
        "\n",
        "def upsert_df_smart(client, collection, df, init_batch=512, id_col=\"id\"):\n",
        "    for col in [id_col, \"embedding\"]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Thiếu cột bắt buộc: {col}\")\n",
        "\n",
        "    vectors_all = ensure_matrix(df[\"embedding\"])\n",
        "\n",
        "    N = len(df)\n",
        "    bs = init_batch\n",
        "    start = 0\n",
        "    while start < N:\n",
        "        end = min(N, start + bs)\n",
        "        part = df.iloc[start:end]\n",
        "\n",
        "        ids = to_uuid5_list(part[id_col], prefix=collection)\n",
        "        vecs = vectors_all[start:end]\n",
        "        payloads = make_payloads(part, id_col=id_col)\n",
        "\n",
        "        try:\n",
        "            client.upsert(\n",
        "                collection_name=collection,\n",
        "                points=Batch(ids=ids, vectors=vecs, payloads=payloads),\n",
        "                wait=True\n",
        "            )\n",
        "            start = end\n",
        "            if bs < init_batch:\n",
        "                bs = min(init_batch, int(bs * 2))\n",
        "        except UnexpectedResponse as e:\n",
        "            msg = str(e)\n",
        "            if any(key in msg for key in [\n",
        "                \"larger than allowed\", \"Payload error\",\n",
        "                \"Request Entity Too Large\", \"413\", \"400\"\n",
        "            ]):\n",
        "                new_bs = max(64, bs // 2)\n",
        "                print(f\"Request quá lớn (window={end-start}, bs={bs}) --- giảm xuống {new_bs}\", file=sys.stderr)\n",
        "                if new_bs == bs:\n",
        "                    raise RuntimeError(\"Batch vẫn vượt giới hạn.\") from e\n",
        "                bs = new_bs\n",
        "            else:\n",
        "                traceback.print_exc()\n",
        "                raise\n",
        "\n",
        "    print(f\"Upsert xong: {collection} (rows={N})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4KfHesJknOu",
        "outputId": "8e92103a-39c1-4411-802c-e50127f4939f"
      },
      "outputs": [],
      "source": [
        "print(\"Upsert FAQ:\")\n",
        "upsert_df_smart(client, \"faq_bge\", faq_df, init_batch=512, id_col=\"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBncP31-kpO7",
        "outputId": "f9417d38-7abf-4e8c-9ebc-d450940c22af"
      },
      "outputs": [],
      "source": [
        "print(\"Upsert Product:\")\n",
        "upsert_df_smart(client,\"product_bge\", prod_df, init_batch=512, id_col=\"id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XSlvEjXRMY2"
      },
      "source": [
        "## Create payload index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1oQJCtkchUz",
        "outputId": "dbd191e5-f6f2-4814-a6ea-47f167de1703"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=179, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from qdrant_client.models import PayloadSchemaType\n",
        "client.create_payload_index(\n",
        "    collection_name=\"product_bge\",\n",
        "    field_name=\"source_id\",\n",
        "    field_schema=PayloadSchemaType.KEYWORD,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a1Z6ufbgSDL",
        "outputId": "97aacb98-f4f0-43c3-d2ce-36f5e19ab6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created payload index: category (keyword)\n",
            "Created payload index: brand (keyword)\n",
            "Created payload index: parent_uid (keyword)\n",
            "Created payload index: type (keyword)\n",
            "Created payload index: rating (float)\n",
            "Created payload index: price_numeric (float)\n",
            "Created payload index: review_count (integer)\n"
          ]
        }
      ],
      "source": [
        "for field, schema in [\n",
        "    (\"category\", PayloadSchemaType.KEYWORD),\n",
        "    (\"brand\", PayloadSchemaType.KEYWORD),\n",
        "    (\"parent_uid\", PayloadSchemaType.KEYWORD),\n",
        "    (\"type\", PayloadSchemaType.KEYWORD),\n",
        "    (\"rating\", PayloadSchemaType.FLOAT),\n",
        "    (\"price_numeric\", PayloadSchemaType.FLOAT),\n",
        "    (\"review_count\", PayloadSchemaType.INTEGER),\n",
        "]:\n",
        "    try:\n",
        "        client.create_payload_index(\n",
        "            collection_name=\"product_bge\",\n",
        "            field_name=field,\n",
        "            field_schema=schema,\n",
        "        )\n",
        "        print(f\"Created payload index: {field} ({schema})\")\n",
        "    except Exception as e:\n",
        "        print(f\"{field} → {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndwsQBpoRWr3"
      },
      "source": [
        "## Create sparse vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjAwvmoL8x3B"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "from collections import defaultdict\n",
        "import os, json, math, re, unicodedata, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUZNcQr3OdpU"
      },
      "outputs": [],
      "source": [
        "def strip_accents(s):\n",
        "    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "def norm_space(s):\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def normalize_text_for_sparse(s):\n",
        "    s = strip_accents(s).lower()\n",
        "    s = re.sub(r\"[^\\w\\s\\-\\%\\./]\", \" \", s)\n",
        "    return norm_space(s)\n",
        "\n",
        "def unify_number(x):\n",
        "    return float(x.replace(\",\", \".\").replace(\" \", \"\"))\n",
        "\n",
        "def mm_from(value, unit):\n",
        "    unit = (unit or \"mm\").lower()\n",
        "    if unit == \"cm\":\n",
        "        return value * 10\n",
        "    if unit == \"m\":\n",
        "        return value * 1000\n",
        "    return value\n",
        "\n",
        "def g_from(value, unit):\n",
        "    unit = unit.lower()\n",
        "    if unit == \"kg\":\n",
        "        return value * 1000\n",
        "    return value\n",
        "\n",
        "def capacity_ml(value, unit):\n",
        "    unit = unit.lower()\n",
        "    if unit == \"l\":\n",
        "        return value * 1000\n",
        "    return value\n",
        "\n",
        "RE_DIM_VI = re.compile(\n",
        "    r\"(?:(?:[cclwnhdrt]\\s*)?)(\\d+(?:[.,]\\d+)?)\\s*[x×]\\s*\"\n",
        "    r\"(?:(?:[cclwnhdrt]\\s*)?)(\\d+(?:[.,]\\d+)?)\\s*[x×]\\s*\"\n",
        "    r\"(?:(?:[cclwnhdrt]\\s*)?)(\\d+(?:[.,]\\d+)?)(?:\\s*(mm|cm|m))?\",\n",
        "    re.I\n",
        ")\n",
        "RE_INCH = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(?:\\\"|inch|in)\\b\", re.I)\n",
        "RE_INCH_RANGE = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*[-–]\\s*(\\d+(?:[.,]\\d+)?)\\s*(?:inch|\\\"|\\b in\\b)\", re.I)\n",
        "\n",
        "RE_CAP_ML = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(ml|l)\\b\", re.I)\n",
        "RE_MAH    = re.compile(r\"(\\d{3,6})\\s*mah\\b\", re.I)\n",
        "RE_WEIGHT = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(kg|g)\\b\", re.I)\n",
        "RE_POWER  = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(w|kw)\\b\", re.I)\n",
        "RE_VOLT   = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*v\\b\", re.I)\n",
        "RE_FREQ   = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*hz\\b\", re.I)\n",
        "RE_RAM     = re.compile(r\"(\\d{1,3})\\s*gb\\s*(?:ram)?\\b\", re.I)\n",
        "RE_STORAGE = re.compile(r\"(\\d{1,4})\\s*(tb|gb)\\s*(?:ssd|hdd)?\\b\", re.I)\n",
        "\n",
        "RE_RES    = re.compile(r\"(\\d{3,5})\\s*[x×]\\s*(\\d{3,5})\\s*(px|pixel)?\", re.I)\n",
        "\n",
        "RE_MODEL_LINE = re.compile(r\"(?:ma\\s*sp|m[aã]\\s*sp|model(?:\\s*no\\.)?|sku)\\s*[:\\-]?\\s*([A-Z0-9\\-\\._]{3,})\", re.I)\n",
        "RE_MODEL_FREE = re.compile(r\"\\b([A-Z0-9]{3,}[-_\\.]?[A-Z0-9]{2,})\\b\")\n",
        "\n",
        "RE_COLOR = re.compile(r\"\\b(den|đen|do|đỏ|xanh|xam|xám|trang|trắng|hong|hồng|vang|vàng|nau|nâu|tim|tím|bac|bạc|gold|silver|black|white|blue|red|green|gray)\\b\", re.I)\n",
        "RE_MATERIAL = re.compile(r\"\\b(oxford|inox|nhua|nhựa|vai|vải|da|polyester|nylon|aluminum|nhom|nhôm|thep|thép|thuy\\s*tinh|thủy\\s*tinh)\\b\", re.I)\n",
        "\n",
        "RE_MARKETING = re.compile(\n",
        "    r\"(cam\\s*k[eê]t|dich\\s*vu|doi\\s*tra|khuyen\\s*mai|cs[kh]|5\\*|phuc\\s*vu|hai\\s*long|lien\\s*he|cua\\s*hang)\",\n",
        "    re.I\n",
        ")\n",
        "TECH_PAT = re.compile(\n",
        "    r\"(\\d+(?:[.,]\\d+)?\\s*(mm|cm|m|inch|\\\"|ml|mah|w|hz|v|gb|tb|px)\\b)\"\n",
        "    r\"|(\\b(kich\\s*thuoc|ma\\s*sp|m[aã]\\s*sp|model|sku|chat\\s*lieu|mau\\s*sac|dung\\s*tich|trong\\s*luong|man\\s*hinh|ram|rom|ssd|hdd|tan\\s*so|dien\\s*ap)\\b)\",\n",
        "    re.I\n",
        ")\n",
        "\n",
        "def split_details_block(text):\n",
        "    base = strip_accents(text)\n",
        "    m = re.search(r\"details\\s*:\\s*\", base, re.I)\n",
        "    if not m:\n",
        "        return \"\", text\n",
        "    start = m.end()\n",
        "    return text[start:], text[:start]\n",
        "\n",
        "def make_text_lite(title, text):\n",
        "    details, _ = split_details_block(text)\n",
        "    cand = f\"{title}\\n{details or text}\"\n",
        "    lines = [norm_space(l) for l in cand.split(\"\\n\") if l.strip()]\n",
        "    out = []\n",
        "    for l in lines:\n",
        "        l_noacc = strip_accents(l.lower())\n",
        "        if RE_MARKETING.search(l_noacc):\n",
        "            continue\n",
        "        if TECH_PAT.search(l_noacc):\n",
        "            out.append(l)\n",
        "    if not out:\n",
        "        out = lines[:3]\n",
        "    return \". \".join(out[:30])\n",
        "\n",
        "def extract_slots(title, text):\n",
        "    raw = f\"{title}\\n{text}\"\n",
        "    raw_noacc_lower = strip_accents(raw).lower()\n",
        "    raw_norm = normalize_text_for_sparse(raw)\n",
        "\n",
        "    slots = {}\n",
        "\n",
        "    m = RE_DIM_VI.search(strip_accents(raw))\n",
        "    if m:\n",
        "        a, b, c, u = m.groups()\n",
        "        a, b, c = map(unify_number, (a, b, c))\n",
        "        slots[\"dimensions_mm\"] = [mm_from(a, u), mm_from(b, u), mm_from(c, u)]\n",
        "        slots[\"dimensions_raw\"] = f\"{m.group(1)}x{m.group(2)}x{m.group(3)} {u or 'mm'}\"\n",
        "\n",
        "    m = RE_INCH.search(raw_norm)\n",
        "    if m:\n",
        "        slots[\"screen_size_inch\"] = unify_number(m.group(1))\n",
        "    m = RE_INCH_RANGE.search(strip_accents(raw))\n",
        "    if m:\n",
        "        lo, hi = map(unify_number, m.groups())\n",
        "        slots[\"compat_inch_range\"] = [min(lo, hi), max(lo, hi)]\n",
        "\n",
        "    m = RE_CAP_ML.search(raw_norm)\n",
        "    if m:\n",
        "        v, u = m.groups()\n",
        "        slots[\"capacity_ml\"] = capacity_ml(unify_number(v), u)\n",
        "        slots[\"capacity_raw\"] = f\"{v}{u}\"\n",
        "    m = RE_MAH.search(raw_norm)\n",
        "    if m:\n",
        "        v = m.group(1)\n",
        "        slots[\"battery_mah\"] = int(unify_number(v))\n",
        "\n",
        "    m = RE_WEIGHT.search(raw_norm)\n",
        "    if m:\n",
        "        v, u = m.groups()\n",
        "        slots[\"weight_g\"] = g_from(unify_number(v), u)\n",
        "        slots[\"weight_raw\"] = f\"{v}{u}\"\n",
        "\n",
        "    for rex, key in [(RE_POWER, \"power_w\"), (RE_VOLT, \"voltage_v\"), (RE_FREQ, \"frequency_hz\")]:\n",
        "        m = rex.search(raw_norm)\n",
        "        if m:\n",
        "            slots[key] = float(unify_number(m.group(1)))\n",
        "\n",
        "    m = RE_RAM.search(raw_norm)\n",
        "    if m:\n",
        "        slots[\"ram_gb\"] = int(unify_number(m.group(1)))\n",
        "    m = RE_STORAGE.search(raw_norm)\n",
        "    if m:\n",
        "        v, unit = m.groups()\n",
        "        val = float(unify_number(v))\n",
        "        slots[\"storage_gb\"] = val * 1024 if unit.lower() == \"tb\" else val\n",
        "\n",
        "    m = RE_RES.search(raw_norm)\n",
        "    if m:\n",
        "        w, h, _ = m.groups()\n",
        "        slots[\"resolution\"] = f\"{w}x{h}\"\n",
        "\n",
        "    m = RE_MODEL_LINE.search(strip_accents(raw))\n",
        "    if m:\n",
        "        slots[\"model\"] = m.group(1).upper()\n",
        "        slots[\"model_conf\"] = \"high\"\n",
        "    else:\n",
        "        if any(k in raw_noacc_lower for k in [\"ma sp\", \"mã sp\", \"model\", \"sku\"]):\n",
        "            m2 = RE_MODEL_FREE.search(strip_accents(raw).upper())\n",
        "            if m2:\n",
        "                slots[\"model\"] = m2.group(1)\n",
        "                slots[\"model_conf\"] = \"medium\"\n",
        "\n",
        "    colors = []\n",
        "    color_line = re.search(r\"(m[aà]u\\s*s[aă]c\\s*:\\s*)(.+)\", strip_accents(raw), re.I)\n",
        "    if color_line:\n",
        "        payload = color_line.group(2)\n",
        "        for tok in re.split(r\"[\\/,\\;\\|]\", payload):\n",
        "            t = norm_space(strip_accents(tok).lower())\n",
        "            if t:\n",
        "                colors.append(t)\n",
        "    if not colors:\n",
        "        colors = list(set(RE_COLOR.findall(strip_accents(raw))))\n",
        "    if colors:\n",
        "        slots[\"color\"] = [c.replace(\" \", \"_\") for c in colors]\n",
        "\n",
        "    materials = []\n",
        "    mat_line = re.search(r\"(ch[aă]t\\s*li[eê]u\\s*:\\s*)(.+)\", strip_accents(raw), re.I)\n",
        "    if mat_line:\n",
        "        payload = mat_line.group(2)\n",
        "        mats = RE_MATERIAL.findall(strip_accents(payload).lower())\n",
        "        if mats:\n",
        "            materials = mats\n",
        "        else:\n",
        "            cand = re.findall(r\"[a-zA-Z]{3,}\", strip_accents(payload))\n",
        "            materials = cand[:2]\n",
        "    else:\n",
        "        mats2 = RE_MATERIAL.findall(raw_noacc_lower)\n",
        "        if mats2:\n",
        "            materials = list(set(mats2))\n",
        "    if materials:\n",
        "        slots[\"material\"] = [m.lower() for m in materials]\n",
        "\n",
        "    feats = []\n",
        "    rn = raw_noacc_lower\n",
        "    if (\"chong tham\" in rn) or (\"chong nuoc\" in rn):\n",
        "        feats.append(\"chong_tham\")\n",
        "    if \"chong soc\" in rn:\n",
        "        feats.append(\"chong_soc\")\n",
        "    if (\"gan len thanh keo\" in rn) or (\"gan vali\" in rn):\n",
        "        feats.append(\"gan_vali\")\n",
        "    if \"dem to ong\" in rn:\n",
        "        feats.append(\"dem_to_ong\")\n",
        "    if feats:\n",
        "        slots[\"features\"] = feats\n",
        "\n",
        "    return slots\n",
        "\n",
        "def make_kv_compact(slots, brand=None, category_norm=None):\n",
        "    parts = []\n",
        "    if brand:\n",
        "        parts.append(f\"brand:{strip_accents(brand).lower()}\")\n",
        "    if category_norm:\n",
        "        parts.append(f\"category:{strip_accents(category_norm).lower()}\")\n",
        "    if \"model\" in slots:\n",
        "        parts.append(f\"model:{slots['model']}\")\n",
        "    if \"dimensions_mm\" in slots:\n",
        "        L, W, H = [int(v) if float(v).is_integer() else round(float(v), 1) for v in slots[\"dimensions_mm\"]]\n",
        "        parts.append(f\"size_mm:{L}x{W}x{H}\")\n",
        "    if \"compat_inch_range\" in slots:\n",
        "        lo, hi = slots[\"compat_inch_range\"]\n",
        "        parts.append(f\"compat_inch:{lo}-{hi}\")\n",
        "    if \"screen_size_inch\" in slots:\n",
        "        parts.append(f\"screen:{slots['screen_size_inch']:.1f}inch\")\n",
        "    if \"capacity_ml\" in slots:\n",
        "        parts.append(f\"capacity_ml:{int(slots['capacity_ml'])}\")\n",
        "    if \"battery_mah\" in slots:\n",
        "        parts.append(f\"battery:{int(slots['battery_mah'])}mah\")\n",
        "    if \"weight_g\" in slots:\n",
        "        parts.append(f\"weight_g:{int(slots['weight_g'])}\")\n",
        "    if \"power_w\" in slots:\n",
        "        parts.append(f\"power_w:{slots['power_w']}\")\n",
        "    if \"voltage_v\" in slots:\n",
        "        parts.append(f\"voltage_v:{slots['voltage_v']}\")\n",
        "    if \"frequency_hz\" in slots:\n",
        "        parts.append(f\"freq_hz:{slots[\"frequency_hz\"]}\")\n",
        "    if \"ram_gb\" in slots:\n",
        "        parts.append(f\"ram:{int(slots['ram_gb'])}gb\")\n",
        "    if \"storage_gb\" in slots:\n",
        "        val = float(slots[\"storage_gb\"])\n",
        "        parts.append(f\"storage:{int(val) if val.is_integer() else val}gb\")\n",
        "    if \"resolution\" in slots:\n",
        "        parts.append(f\"res:{slots['resolution']}\")\n",
        "    if \"material\" in slots:\n",
        "        parts.append(\"material:\" + \"_\".join(slots[\"material\"]))\n",
        "    if \"color\" in slots:\n",
        "        parts.append(\"color:\" + \"_\".join(slots[\"color\"]))\n",
        "    if \"features\" in slots:\n",
        "        parts.append(\"feat:\" + \"_\".join(slots[\"features\"]))\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "def make_text_dense_input(record, category_norm=None):\n",
        "    title = record.get(\"title\", \"\") or \"\"\n",
        "    text = record.get(\"text\", \"\") or \"\"\n",
        "    brand = record.get(\"brand\")\n",
        "    catn = category_norm or record.get(\"category\")\n",
        "    slots = extract_slots(title, text)\n",
        "    kv = make_kv_compact(slots, brand=brand, category_norm=catn)\n",
        "    text_lite = make_text_lite(title, text)\n",
        "    parts = [title]\n",
        "    if brand:\n",
        "        parts.append(f\"Brand: {brand}\")\n",
        "    if catn:\n",
        "        parts.append(f\"Category: {catn}\")\n",
        "    if kv:\n",
        "        parts.append(kv)\n",
        "    if text_lite:\n",
        "        parts.append(text_lite)\n",
        "    return \"\\n\".join([p for p in parts if p])\n",
        "\n",
        "def make_text_sparse_input(record, category_norm=None):\n",
        "    title = record.get(\"title\", \"\") or \"\"\n",
        "    text = record.get(\"text\", \"\") or \"\"\n",
        "    brand = record.get(\"brand\")\n",
        "    catn = category_norm or record.get(\"category\")\n",
        "    slots = extract_slots(title, text)\n",
        "    kv = make_kv_compact(slots, brand=brand, category_norm=catn)\n",
        "    text_lite = make_text_lite(title, text)\n",
        "    raw = \" | \".join([\n",
        "        kv,\n",
        "        title,\n",
        "        brand or \"\",\n",
        "        text_lite\n",
        "    ])\n",
        "    return normalize_text_for_sparse(raw)\n",
        "\n",
        "def build_product_texts(record, category_norm=None):\n",
        "    title = record.get(\"title\", \"\") or \"\"\n",
        "    text = record.get(\"text\", \"\") or \"\"\n",
        "    brand = record.get(\"brand\")\n",
        "    catn = category_norm or record.get(\"category\")\n",
        "    slots = extract_slots(title, text)\n",
        "    kv = make_kv_compact(slots, brand=brand, category_norm=catn)\n",
        "    text_lite = make_text_lite(title, text)\n",
        "    dense_in = make_text_dense_input(record, category_norm=catn)\n",
        "    sparse_in = make_text_sparse_input(record, category_norm=catn)\n",
        "    return {\n",
        "        \"text_dense\": dense_in,\n",
        "        \"text_sparse\": sparse_in,\n",
        "        \"kv_compact\": kv,\n",
        "        \"text_lite\": text_lite,\n",
        "        \"slots\": slots,\n",
        "    }\n",
        "\n",
        "def _uniq(seq):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if x and x not in seen:\n",
        "            seen.add(x)\n",
        "            out.append(x)\n",
        "    return out\n",
        "\n",
        "def build_parent_sparse_from_chunks(rows,\n",
        "                                    category_norm_map=None,\n",
        "                                    title_boost=1.5, brand_boost=1.5, kv_boost=2.0,\n",
        "                                    max_titles=3, max_text_lite_chars=4000):\n",
        "    buckets = defaultdict(list)\n",
        "    for r in rows:\n",
        "        buckets[r[\"parent_uid\"]].append(r)\n",
        "\n",
        "    out = {}\n",
        "    for pid, items in buckets.items():\n",
        "        titles = _uniq([i.get(\"title\", \"\") for i in items])\n",
        "        texts = [i.get(\"text\", \"\") for i in items]\n",
        "        brand = next((i.get(\"brand\") for i in items if i.get(\"brand\")), \"\")\n",
        "        cat_raw = next((i.get(\"category\") for i in items if i.get(\"category\")), \"\")\n",
        "        catn = category_norm_map.get(cat_raw, cat_raw) if category_norm_map else cat_raw\n",
        "\n",
        "        kv_all, lite_all = [], []\n",
        "        for it in items:\n",
        "            slots = extract_slots(it.get(\"title\", \"\"), it.get(\"text\", \"\"))\n",
        "            kv = make_kv_compact(slots, brand=brand, category_norm=catn)\n",
        "            lite = make_text_lite(it.get(\"title\", \"\"), it.get(\"text\", \"\"))\n",
        "            if kv:\n",
        "                kv_all.append(kv)\n",
        "            if lite:\n",
        "                lite_all.append(lite)\n",
        "\n",
        "        kv_merged = \" | \".join(_uniq(kv_all))\n",
        "        lite_merged = \". \".join(_uniq(lite_all))[:max_text_lite_chars]\n",
        "\n",
        "        title_part = (\" | \".join(titles[:max_titles]) + \" \") * int(title_boost)\n",
        "        brand_part = ((brand or \"\") + \" \") * int(brand_boost)\n",
        "        kv_part = (kv_merged + \" \") * int(kv_boost)\n",
        "\n",
        "        raw_sparse = \" | \".join([\n",
        "            kv_part.strip(),\n",
        "            title_part.strip(),\n",
        "            brand_part.strip(),\n",
        "            lite_merged\n",
        "        ])\n",
        "\n",
        "        text_sparse_parent = normalize_text_for_sparse(raw_sparse)\n",
        "\n",
        "        out[pid] = {\n",
        "            \"text_sparse_parent\": text_sparse_parent,\n",
        "            \"kv_compact\": kv_merged,\n",
        "            \"brand\": brand,\n",
        "            \"category_norm\": catn\n",
        "        }\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnCYk9HRRl8t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_chunks = pd.read_json(\n",
        "    '/content/drive/MyDrive/tiki_chatbot/products_chunked.enriched.jsonl',\n",
        "    lines=True\n",
        ")\n",
        "\n",
        "COLS_TO_STR = [\"title\", \"text\", \"brand\", \"category\", \"parent_uid\"]\n",
        "df_chunks[COLS_TO_STR] = df_chunks[COLS_TO_STR].fillna(\"\")\n",
        "\n",
        "\n",
        "try:\n",
        "    parent_sparse = build_parent_sparse_from_chunks(df_chunks.to_dict(\"records\"))\n",
        "    print(\"Done.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi: {e}\")\n",
        "\n",
        "parent_sparse = build_parent_sparse_from_chunks(df_chunks.to_dict(\"records\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKiAQdHkRoPq"
      },
      "outputs": [],
      "source": [
        "def strip_accents(s):\n",
        "    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "def normalize_text_for_sparse(s):\n",
        "    s = strip_accents(str(s)).lower()\n",
        "    s = re.sub(r\"[^\\w\\s\\-\\%\\./]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenize(s):\n",
        "    s = normalize_text_for_sparse(s)\n",
        "    toks = s.split()\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if len(t) == 1 and not t.isdigit():\n",
        "            continue\n",
        "        out.append(t)\n",
        "    return out\n",
        "\n",
        "\n",
        "class BM25Okapi:\n",
        "    def __init__(self, corpus_tokens, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.corpus_size = len(corpus_tokens)\n",
        "        self.doc_len = np.array([len(doc) for doc in corpus_tokens], dtype=np.float32)\n",
        "        self.avgdl = float(self.doc_len.mean()) if self.corpus_size else 0.0\n",
        "\n",
        "        self.term_freqs = []\n",
        "        self.doc_freqs = {}\n",
        "        for doc in corpus_tokens:\n",
        "            tf = {}\n",
        "            for w in doc:\n",
        "                tf[w] = tf.get(w, 0) + 1\n",
        "            self.term_freqs.append(tf)\n",
        "            for w in tf.keys():\n",
        "                self.doc_freqs[w] = self.doc_freqs.get(w, 0) + 1\n",
        "\n",
        "        self.idf = {}\n",
        "        for w, df in self.doc_freqs.items():\n",
        "            val = math.log((self.corpus_size - df + 0.5) / (df + 0.5) + 1e-9)\n",
        "            if val < 0:\n",
        "                val *= self.epsilon\n",
        "            self.idf[w] = val\n",
        "\n",
        "    def get_scores(self, query_tokens):\n",
        "        scores = np.zeros(self.corpus_size, dtype=np.float32)\n",
        "        if self.corpus_size == 0:\n",
        "            return scores\n",
        "        for w in query_tokens:\n",
        "            if w not in self.idf:\n",
        "                continue\n",
        "            idf = self.idf[w]\n",
        "            for i, tf in enumerate(self.term_freqs):\n",
        "                f = tf.get(w, 0)\n",
        "                if f == 0:\n",
        "                    continue\n",
        "                denom = f + self.k1 * (1 - self.b + self.b * (self.doc_len[i] / (self.avgdl or 1.0)))\n",
        "                scores[i] += idf * (f * (self.k1 + 1)) / (denom + 1e-9)\n",
        "        return scores\n",
        "\n",
        "class ParentBM25Index:\n",
        "    def __init__(self, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.parents = []\n",
        "        self.meta = {}\n",
        "        self.docs_tokens = []\n",
        "        self._bm25 = None\n",
        "\n",
        "    def fit(self, parent_rows):\n",
        "        self.parents.clear()\n",
        "        self.meta = {}\n",
        "        self.docs_tokens.clear()\n",
        "\n",
        "        for pid, info in parent_rows.items():\n",
        "            ts = (info.get(\"text_sparse_parent\") or \"\").strip()\n",
        "            if not ts:\n",
        "                continue\n",
        "            self.parents.append(pid)\n",
        "            self.meta[pid] = info\n",
        "            self.docs_tokens.append(tokenize(ts))\n",
        "\n",
        "        self._bm25 = BM25Okapi(self.docs_tokens, k1=self.k1, b=self.b, epsilon=self.epsilon)\n",
        "\n",
        "    def is_ready(self):\n",
        "        return (self._bm25 is not None) and (len(self.parents) == len(self.docs_tokens) > 0)\n",
        "\n",
        "    def search(self, query, topk=20):\n",
        "        if not self.is_ready():\n",
        "            return pd.DataFrame(columns=[\"parent_uid\", \"score\", \"brand\", \"category_norm\", \"kv_compact\"])\n",
        "\n",
        "        q_tokens = tokenize(query)\n",
        "        scores = self._bm25.get_scores(q_tokens)\n",
        "\n",
        "        k = min(topk, len(scores))\n",
        "        idx = np.argpartition(-scores, kth=k-1)[:k]\n",
        "        idx = idx[np.argsort(-scores[idx])]\n",
        "\n",
        "        rows = []\n",
        "        for i in idx:\n",
        "            pid = self.parents[i]\n",
        "            m = self.meta.get(pid, {})\n",
        "            rows.append({\n",
        "                \"parent_uid\": pid,\n",
        "                \"score\": float(scores[i]),\n",
        "                \"brand\": m.get(\"brand\"),\n",
        "                \"category_norm\": m.get(\"category_norm\"),\n",
        "                \"kv_compact\": m.get(\"kv_compact\"),\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def save(self, folder):\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        with open(os.path.join(folder, \"parents.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.parents, f)\n",
        "        with open(os.path.join(folder, \"docs_tokens.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.docs_tokens, f)\n",
        "        with open(os.path.join(folder, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
        "        with open(os.path.join(folder, \"params.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"k1\": self.k1, \"b\": self.b, \"epsilon\": self.epsilon}, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, folder):\n",
        "        with open(os.path.join(folder, \"params.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            p = json.load(f)\n",
        "        obj = cls(k1=p.get(\"k1\", 1.5), b=p.get(\"b\", 0.75), epsilon=p.get(\"epsilon\", 0.25))\n",
        "        with open(os.path.join(folder, \"parents.pkl\"), \"rb\") as f:\n",
        "            obj.parents = pickle.load(f)\n",
        "        with open(os.path.join(folder, \"docs_tokens.pkl\"), \"rb\") as f:\n",
        "            obj.docs_tokens = pickle.load(f)\n",
        "        with open(os.path.join(folder, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            obj.meta = json.load(f)\n",
        "        obj._bm25 = BM25Okapi(obj.docs_tokens, k1=obj.k1, b=obj.b, epsilon=obj.epsilon)\n",
        "        return obj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdXg0yVlSmHT"
      },
      "outputs": [],
      "source": [
        "pidx = ParentBM25Index(k1=1.3, b=0.72)\n",
        "pidx.fit(parent_sparse)\n",
        "\n",
        "pidx.save(\"bm25_parent_index\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
