{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\phogpt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, types, torch, wandb\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.modules[\"bitsandbytes\"] = type(sys)(\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"PhoGPT-Tiki\"\n",
    "WANDB_NAME = \"XXXX\"\n",
    "WANDB_API_KEY = \"XXXX\"\n",
    "MODEL_NAME = \"vinai/PhoGPT-4B\"\n",
    "DATA_PATH = \"tiki_dataset_processed.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "wandb.login()\n",
    "wandb.init(project=WANDB_PROJECT, name=WANDB_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "DATA_PATH = r\"D:\\PHOGPT\\tiki_dataset_processed.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(\"Loaded dataset successfully!\")\n",
    "print(\"Training samples:\", len(dataset[\"train\"]))\n",
    "print(\"Testing samples:\", len(dataset[\"test\"]))\n",
    "print(\"Mẫu đầu tiên:\", dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded | pad_token:\", tokenizer.pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.blocks.0.attn\n",
      "transformer.blocks.0.attn.Wqkv\n",
      "transformer.blocks.0.attn.out_proj\n",
      "transformer.blocks.0.ffn\n",
      "transformer.blocks.0.ffn.up_proj\n",
      "transformer.blocks.0.ffn.down_proj\n",
      "transformer.blocks.0.resid_attn_dropout\n",
      "transformer.blocks.0.resid_ffn_dropout\n",
      "transformer.blocks.1.attn\n",
      "transformer.blocks.1.attn.Wqkv\n",
      "transformer.blocks.1.attn.out_proj\n",
      "transformer.blocks.1.ffn\n",
      "transformer.blocks.1.ffn.up_proj\n",
      "transformer.blocks.1.ffn.down_proj\n",
      "transformer.blocks.1.resid_attn_dropout\n",
      "transformer.blocks.1.resid_ffn_dropout\n",
      "transformer.blocks.2.attn\n",
      "transformer.blocks.2.attn.Wqkv\n",
      "transformer.blocks.2.attn.out_proj\n",
      "transformer.blocks.2.ffn\n",
      "transformer.blocks.2.ffn.up_proj\n",
      "transformer.blocks.2.ffn.down_proj\n",
      "transformer.blocks.2.resid_attn_dropout\n",
      "transformer.blocks.2.resid_ffn_dropout\n",
      "transformer.blocks.3.attn\n",
      "transformer.blocks.3.attn.Wqkv\n",
      "transformer.blocks.3.attn.out_proj\n",
      "transformer.blocks.3.ffn\n",
      "transformer.blocks.3.ffn.up_proj\n",
      "transformer.blocks.3.ffn.down_proj\n",
      "transformer.blocks.3.resid_attn_dropout\n",
      "transformer.blocks.3.resid_ffn_dropout\n",
      "transformer.blocks.4.attn\n",
      "transformer.blocks.4.attn.Wqkv\n",
      "transformer.blocks.4.attn.out_proj\n",
      "transformer.blocks.4.ffn\n",
      "transformer.blocks.4.ffn.up_proj\n",
      "transformer.blocks.4.ffn.down_proj\n",
      "transformer.blocks.4.resid_attn_dropout\n",
      "transformer.blocks.4.resid_ffn_dropout\n",
      "transformer.blocks.5.attn\n",
      "transformer.blocks.5.attn.Wqkv\n",
      "transformer.blocks.5.attn.out_proj\n",
      "transformer.blocks.5.ffn\n",
      "transformer.blocks.5.ffn.up_proj\n",
      "transformer.blocks.5.ffn.down_proj\n",
      "transformer.blocks.5.resid_attn_dropout\n",
      "transformer.blocks.5.resid_ffn_dropout\n",
      "transformer.blocks.6.attn\n",
      "transformer.blocks.6.attn.Wqkv\n",
      "transformer.blocks.6.attn.out_proj\n",
      "transformer.blocks.6.ffn\n",
      "transformer.blocks.6.ffn.up_proj\n",
      "transformer.blocks.6.ffn.down_proj\n",
      "transformer.blocks.6.resid_attn_dropout\n",
      "transformer.blocks.6.resid_ffn_dropout\n",
      "transformer.blocks.7.attn\n",
      "transformer.blocks.7.attn.Wqkv\n",
      "transformer.blocks.7.attn.out_proj\n",
      "transformer.blocks.7.ffn\n",
      "transformer.blocks.7.ffn.up_proj\n",
      "transformer.blocks.7.ffn.down_proj\n",
      "transformer.blocks.7.resid_attn_dropout\n",
      "transformer.blocks.7.resid_ffn_dropout\n",
      "transformer.blocks.8.attn\n",
      "transformer.blocks.8.attn.Wqkv\n",
      "transformer.blocks.8.attn.out_proj\n",
      "transformer.blocks.8.ffn\n",
      "transformer.blocks.8.ffn.up_proj\n",
      "transformer.blocks.8.ffn.down_proj\n",
      "transformer.blocks.8.resid_attn_dropout\n",
      "transformer.blocks.8.resid_ffn_dropout\n",
      "transformer.blocks.9.attn\n",
      "transformer.blocks.9.attn.Wqkv\n",
      "transformer.blocks.9.attn.out_proj\n",
      "transformer.blocks.9.ffn\n",
      "transformer.blocks.9.ffn.up_proj\n",
      "transformer.blocks.9.ffn.down_proj\n",
      "transformer.blocks.9.resid_attn_dropout\n",
      "transformer.blocks.9.resid_ffn_dropout\n",
      "transformer.blocks.10.attn\n",
      "transformer.blocks.10.attn.Wqkv\n",
      "transformer.blocks.10.attn.out_proj\n",
      "transformer.blocks.10.ffn\n",
      "transformer.blocks.10.ffn.up_proj\n",
      "transformer.blocks.10.ffn.down_proj\n",
      "transformer.blocks.10.resid_attn_dropout\n",
      "transformer.blocks.10.resid_ffn_dropout\n",
      "transformer.blocks.11.attn\n",
      "transformer.blocks.11.attn.Wqkv\n",
      "transformer.blocks.11.attn.out_proj\n",
      "transformer.blocks.11.ffn\n",
      "transformer.blocks.11.ffn.up_proj\n",
      "transformer.blocks.11.ffn.down_proj\n",
      "transformer.blocks.11.resid_attn_dropout\n",
      "transformer.blocks.11.resid_ffn_dropout\n",
      "transformer.blocks.12.attn\n",
      "transformer.blocks.12.attn.Wqkv\n",
      "transformer.blocks.12.attn.out_proj\n",
      "transformer.blocks.12.ffn\n",
      "transformer.blocks.12.ffn.up_proj\n",
      "transformer.blocks.12.ffn.down_proj\n",
      "transformer.blocks.12.resid_attn_dropout\n",
      "transformer.blocks.12.resid_ffn_dropout\n",
      "transformer.blocks.13.attn\n",
      "transformer.blocks.13.attn.Wqkv\n",
      "transformer.blocks.13.attn.out_proj\n",
      "transformer.blocks.13.ffn\n",
      "transformer.blocks.13.ffn.up_proj\n",
      "transformer.blocks.13.ffn.down_proj\n",
      "transformer.blocks.13.resid_attn_dropout\n",
      "transformer.blocks.13.resid_ffn_dropout\n",
      "transformer.blocks.14.attn\n",
      "transformer.blocks.14.attn.Wqkv\n",
      "transformer.blocks.14.attn.out_proj\n",
      "transformer.blocks.14.ffn\n",
      "transformer.blocks.14.ffn.up_proj\n",
      "transformer.blocks.14.ffn.down_proj\n",
      "transformer.blocks.14.resid_attn_dropout\n",
      "transformer.blocks.14.resid_ffn_dropout\n",
      "transformer.blocks.15.attn\n",
      "transformer.blocks.15.attn.Wqkv\n",
      "transformer.blocks.15.attn.out_proj\n",
      "transformer.blocks.15.ffn\n",
      "transformer.blocks.15.ffn.up_proj\n",
      "transformer.blocks.15.ffn.down_proj\n",
      "transformer.blocks.15.resid_attn_dropout\n",
      "transformer.blocks.15.resid_ffn_dropout\n",
      "transformer.blocks.16.attn\n",
      "transformer.blocks.16.attn.Wqkv\n",
      "transformer.blocks.16.attn.out_proj\n",
      "transformer.blocks.16.ffn\n",
      "transformer.blocks.16.ffn.up_proj\n",
      "transformer.blocks.16.ffn.down_proj\n",
      "transformer.blocks.16.resid_attn_dropout\n",
      "transformer.blocks.16.resid_ffn_dropout\n",
      "transformer.blocks.17.attn\n",
      "transformer.blocks.17.attn.Wqkv\n",
      "transformer.blocks.17.attn.out_proj\n",
      "transformer.blocks.17.ffn\n",
      "transformer.blocks.17.ffn.up_proj\n",
      "transformer.blocks.17.ffn.down_proj\n",
      "transformer.blocks.17.resid_attn_dropout\n",
      "transformer.blocks.17.resid_ffn_dropout\n",
      "transformer.blocks.18.attn\n",
      "transformer.blocks.18.attn.Wqkv\n",
      "transformer.blocks.18.attn.out_proj\n",
      "transformer.blocks.18.ffn\n",
      "transformer.blocks.18.ffn.up_proj\n",
      "transformer.blocks.18.ffn.down_proj\n",
      "transformer.blocks.18.resid_attn_dropout\n",
      "transformer.blocks.18.resid_ffn_dropout\n",
      "transformer.blocks.19.attn\n",
      "transformer.blocks.19.attn.Wqkv\n",
      "transformer.blocks.19.attn.out_proj\n",
      "transformer.blocks.19.ffn\n",
      "transformer.blocks.19.ffn.up_proj\n",
      "transformer.blocks.19.ffn.down_proj\n",
      "transformer.blocks.19.resid_attn_dropout\n",
      "transformer.blocks.19.resid_ffn_dropout\n",
      "transformer.blocks.20.attn\n",
      "transformer.blocks.20.attn.Wqkv\n",
      "transformer.blocks.20.attn.out_proj\n",
      "transformer.blocks.20.ffn\n",
      "transformer.blocks.20.ffn.up_proj\n",
      "transformer.blocks.20.ffn.down_proj\n",
      "transformer.blocks.20.resid_attn_dropout\n",
      "transformer.blocks.20.resid_ffn_dropout\n",
      "transformer.blocks.21.attn\n",
      "transformer.blocks.21.attn.Wqkv\n",
      "transformer.blocks.21.attn.out_proj\n",
      "transformer.blocks.21.ffn\n",
      "transformer.blocks.21.ffn.up_proj\n",
      "transformer.blocks.21.ffn.down_proj\n",
      "transformer.blocks.21.resid_attn_dropout\n",
      "transformer.blocks.21.resid_ffn_dropout\n",
      "transformer.blocks.22.attn\n",
      "transformer.blocks.22.attn.Wqkv\n",
      "transformer.blocks.22.attn.out_proj\n",
      "transformer.blocks.22.ffn\n",
      "transformer.blocks.22.ffn.up_proj\n",
      "transformer.blocks.22.ffn.down_proj\n",
      "transformer.blocks.22.resid_attn_dropout\n",
      "transformer.blocks.22.resid_ffn_dropout\n",
      "transformer.blocks.23.attn\n",
      "transformer.blocks.23.attn.Wqkv\n",
      "transformer.blocks.23.attn.out_proj\n",
      "transformer.blocks.23.ffn\n",
      "transformer.blocks.23.ffn.up_proj\n",
      "transformer.blocks.23.ffn.down_proj\n",
      "transformer.blocks.23.resid_attn_dropout\n",
      "transformer.blocks.23.resid_ffn_dropout\n",
      "transformer.blocks.24.attn\n",
      "transformer.blocks.24.attn.Wqkv\n",
      "transformer.blocks.24.attn.out_proj\n",
      "transformer.blocks.24.ffn\n",
      "transformer.blocks.24.ffn.up_proj\n",
      "transformer.blocks.24.ffn.down_proj\n",
      "transformer.blocks.24.resid_attn_dropout\n",
      "transformer.blocks.24.resid_ffn_dropout\n",
      "transformer.blocks.25.attn\n",
      "transformer.blocks.25.attn.Wqkv\n",
      "transformer.blocks.25.attn.out_proj\n",
      "transformer.blocks.25.ffn\n",
      "transformer.blocks.25.ffn.up_proj\n",
      "transformer.blocks.25.ffn.down_proj\n",
      "transformer.blocks.25.resid_attn_dropout\n",
      "transformer.blocks.25.resid_ffn_dropout\n",
      "transformer.blocks.26.attn\n",
      "transformer.blocks.26.attn.Wqkv\n",
      "transformer.blocks.26.attn.out_proj\n",
      "transformer.blocks.26.ffn\n",
      "transformer.blocks.26.ffn.up_proj\n",
      "transformer.blocks.26.ffn.down_proj\n",
      "transformer.blocks.26.resid_attn_dropout\n",
      "transformer.blocks.26.resid_ffn_dropout\n",
      "transformer.blocks.27.attn\n",
      "transformer.blocks.27.attn.Wqkv\n",
      "transformer.blocks.27.attn.out_proj\n",
      "transformer.blocks.27.ffn\n",
      "transformer.blocks.27.ffn.up_proj\n",
      "transformer.blocks.27.ffn.down_proj\n",
      "transformer.blocks.27.resid_attn_dropout\n",
      "transformer.blocks.27.resid_ffn_dropout\n",
      "transformer.blocks.28.attn\n",
      "transformer.blocks.28.attn.Wqkv\n",
      "transformer.blocks.28.attn.out_proj\n",
      "transformer.blocks.28.ffn\n",
      "transformer.blocks.28.ffn.up_proj\n",
      "transformer.blocks.28.ffn.down_proj\n",
      "transformer.blocks.28.resid_attn_dropout\n",
      "transformer.blocks.28.resid_ffn_dropout\n",
      "transformer.blocks.29.attn\n",
      "transformer.blocks.29.attn.Wqkv\n",
      "transformer.blocks.29.attn.out_proj\n",
      "transformer.blocks.29.ffn\n",
      "transformer.blocks.29.ffn.up_proj\n",
      "transformer.blocks.29.ffn.down_proj\n",
      "transformer.blocks.29.resid_attn_dropout\n",
      "transformer.blocks.29.resid_ffn_dropout\n",
      "transformer.blocks.30.attn\n",
      "transformer.blocks.30.attn.Wqkv\n",
      "transformer.blocks.30.attn.out_proj\n",
      "transformer.blocks.30.ffn\n",
      "transformer.blocks.30.ffn.up_proj\n",
      "transformer.blocks.30.ffn.down_proj\n",
      "transformer.blocks.30.resid_attn_dropout\n",
      "transformer.blocks.30.resid_ffn_dropout\n",
      "transformer.blocks.31.attn\n",
      "transformer.blocks.31.attn.Wqkv\n",
      "transformer.blocks.31.attn.out_proj\n",
      "transformer.blocks.31.ffn\n",
      "transformer.blocks.31.ffn.up_proj\n",
      "transformer.blocks.31.ffn.down_proj\n",
      "transformer.blocks.31.resid_attn_dropout\n",
      "transformer.blocks.31.resid_ffn_dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if any(k in name.lower() for k in [\"attn\", \"proj\", \"wq\", \"wv\", \"ffn\", \"linear\"]):\n",
    "        print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoGPT-4B loaded successfully\n",
      "trainable params: 4,718,592 || all params: 3,692,795,904 || trainable%: 0.1277783046414471\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"PhoGPT-4B loaded successfully\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"Wqkv\", \"out_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8152/8152 [00:00<00:00, 23741.90 examples/s]\n",
      "Map: 100%|██████████| 906/906 [00:00<00:00, 25495.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenization ---\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./PhoGPT-Tiki-Finetuned\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,                 \n",
    "    logging_steps=200,               \n",
    "    learning_rate=3e-5,               \n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,                \n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=\"wandb\",\n",
    "    gradient_accumulation_steps=2,    \n",
    "    warmup_ratio=0.05,                 \n",
    "    gradient_checkpointing=False,      \n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./PhoGPT-Tiki-Finetuned\")\n",
    "tokenizer.save_pretrained(\"./PhoGPT-Tiki-Finetuned\")\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
